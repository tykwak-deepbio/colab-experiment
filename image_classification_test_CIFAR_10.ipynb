{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image classification test CIFAR-10",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tykwak-deepbio/colab-experiment/blob/master/image_classification_test_CIFAR_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY8ZRmYe6bE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import datetime\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWLqwOK49D4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SE2d(nn.Module):\n",
        "    def __init__(self, in_channel, intra_channel):\n",
        "        super(SE2d, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channel, intra_channel, 1)\n",
        "        self.conv2 = nn.Conv2d(intra_channel, in_channel, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a = F.avg_pool2d(x, x.size()[2:])\n",
        "        a = F.relu(self.conv1(a))\n",
        "        a = torch.sigmoid(self.conv2(a))\n",
        "        x = x * a.expand_as(x)\n",
        "        return x\n",
        "\n",
        "class AllConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AllConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 5, padding=2)\n",
        "        self.bn1   = nn.BatchNorm2d(64)\n",
        "        self.se1   = SE2d(64, 8)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1, stride=2)\n",
        "        self.drop2 = nn.Dropout2d(p=0.5)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 5, padding=2)\n",
        "        self.bn3   = nn.BatchNorm2d(128)\n",
        "        self.se3   = SE2d(128, 16)\n",
        "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1, stride=2)\n",
        "        self.drop4 = nn.Dropout2d(p=0.5)\n",
        "        self.conv5 = nn.Conv2d(128, 256, 5, padding=2)\n",
        "        self.bn5   = nn.BatchNorm2d(256)\n",
        "        self.se5   = SE2d(256, 32)\n",
        "        self.conv6 = nn.Conv2d(256, 10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.se1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.drop2(x)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.se3(x)\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.drop4(x)\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = self.se5(x)\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = F.avg_pool2d(x, x.size()[2:]).squeeze(-1).squeeze(-1)\n",
        "        return x\n",
        "\n",
        "class MeanTeacher(nn.Module):\n",
        "    def __init__(self, model, var1=0.0001, var2=0.0001, weight=0.9):\n",
        "        assert var1 >= 0\n",
        "        assert var2 >= 0\n",
        "        assert weight > 0 and weight < 1\n",
        "        super(MeanTeacher, self).__init__()\n",
        "        self.student = model\n",
        "        self.teacher = (model.__class__)()\n",
        "        self.stdev1 = math.sqrt(var1)\n",
        "        self.stdev2 = math.sqrt(var2)\n",
        "        self.weight = weight\n",
        "        # copy student to teacher\n",
        "        dict_student = self.student.state_dict()\n",
        "        dict_teacher = self.teacher.state_dict()\n",
        "        for key in dict_teacher:\n",
        "            dict_teacher[key] = dict_student[key] \n",
        "        self.teacher.load_state_dict(dict_teacher)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            self.teacher.requires_grad = False\n",
        "            size = x.size()\n",
        "            x1 = x + torch.randn(size, device=x.device) * self.stdev1    # TODO: check model device\n",
        "            x2 = x + torch.randn(size, device=x.device) * self.stdev2    # TODO: check model device\n",
        "            y1 = self.student(x1)\n",
        "            y2 = self.teacher(x2)\n",
        "            cl = (y2 - y1).pow(2).sum()\n",
        "            return y1, cl\n",
        "        else:\n",
        "            y = self.teacher(x)\n",
        "            return y, 0\n",
        "\n",
        "    def update(self):\n",
        "        if self.training:\n",
        "            dict_student = self.student.state_dict()\n",
        "            dict_teacher = self.teacher.state_dict()\n",
        "            for key in dict_teacher:\n",
        "                dict_teacher[key] = dict_teacher[key] * self.weight + dict_student[key] * (1 - self.weight)\n",
        "            self.teacher.load_state_dict(dict_teacher)\n",
        "\n",
        "net = MeanTeacher(AllConvNet(), var1=0.0001, var2=0.0001, weight=0.99).cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5EOX1bsQF4C",
        "colab_type": "code",
        "outputId": "c546aff2-9200-4d19-b783-064bafd8d83c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "mean = (0.4913997551666284, 0.48215855929893703, 0.4465309133731618)\n",
        "std = (0.24703225141799082, 0.24348516474564, 0.26158783926049628)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=5),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4, nesterov=True)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150, 200], gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/170498071 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:01, 87701024.05it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsYbx3KLRdHd",
        "colab_type": "code",
        "outputId": "1bf3305e-a310-4141-ad81-5134298981ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "timestamp = str(datetime.datetime.now()).split('.')\n",
        "print('(%s) Training Started' % timestamp[0])\n",
        "\n",
        "def wl(epoch):\n",
        "    if epoch < 10:\n",
        "        return 0.01\n",
        "    elif epoch < 20:\n",
        "        return 0.02\n",
        "    elif epoch < 30:\n",
        "        return 0.04\n",
        "    elif epoch < 40:\n",
        "        return 0.08\n",
        "    elif epoch < 50:\n",
        "        return 0.16\n",
        "    elif epoch < 60:\n",
        "        return 0.32\n",
        "    elif epoch < 70:\n",
        "        return 0.64\n",
        "    else:\n",
        "        return 1.0\n",
        "\n",
        "for epoch in range(250):\n",
        "    scheduler.step()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    net.train()\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        outputs, loss2 = net(inputs)\n",
        "        if i % 10 < 1:\n",
        "            loss = criterion(outputs, labels) + wl(epoch) * loss2\n",
        "        else:\n",
        "            loss = wl(epoch) * loss2\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        net.update()\n",
        "        running_loss += loss.item()\n",
        "        total += labels.size(0)\n",
        "    timestamp = str(datetime.datetime.now()).split('.')\n",
        "    print('(%s) [%03d] {Tr} loss: %.6f' % (timestamp[0], epoch + 1, 4 * running_loss / total), end='')\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    net.eval()\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs, loss2 = net(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    print(' {Va} loss: %.6f acc: %.2f%%' % (128 * running_loss / total, 100.0 * correct / total))\n",
        "\n",
        "timestamp = str(datetime.datetime.now()).split('.')\n",
        "print('(%s) Training Finished' % timestamp[0])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2019-06-14 02:53:37) Training Started\n",
            "(2019-06-14 02:57:14) [001] {Tr} loss: 0.229345 {Va} loss: 2.255415 acc: 17.23%\n",
            "(2019-06-14 03:00:56) [002] {Tr} loss: 0.226892 {Va} loss: 2.232613 acc: 19.15%\n",
            "(2019-06-14 03:04:38) [003] {Tr} loss: 0.226528 {Va} loss: 2.233479 acc: 19.41%\n",
            "(2019-06-14 03:08:20) [004] {Tr} loss: 0.226083 {Va} loss: 2.225054 acc: 20.01%\n",
            "(2019-06-14 03:12:01) [005] {Tr} loss: 0.225124 {Va} loss: 2.208807 acc: 20.11%\n",
            "(2019-06-14 03:15:43) [006] {Tr} loss: 0.225321 {Va} loss: 2.202458 acc: 22.45%\n",
            "(2019-06-14 03:19:26) [007] {Tr} loss: 0.224510 {Va} loss: 2.190380 acc: 22.62%\n",
            "(2019-06-14 03:23:08) [008] {Tr} loss: 0.223842 {Va} loss: 2.163603 acc: 23.59%\n",
            "(2019-06-14 03:26:50) [009] {Tr} loss: 0.222468 {Va} loss: 2.146799 acc: 23.03%\n",
            "(2019-06-14 03:30:32) [010] {Tr} loss: 0.222092 {Va} loss: 2.142327 acc: 23.80%\n",
            "(2019-06-14 03:34:15) [011] {Tr} loss: 0.222109 {Va} loss: 2.153796 acc: 24.18%\n",
            "(2019-06-14 03:37:58) [012] {Tr} loss: 0.222412 {Va} loss: 2.149233 acc: 23.33%\n",
            "(2019-06-14 03:41:41) [013] {Tr} loss: 0.221610 {Va} loss: 2.139051 acc: 24.44%\n",
            "(2019-06-14 03:45:25) [014] {Tr} loss: 0.221434 {Va} loss: 2.136607 acc: 20.88%\n",
            "(2019-06-14 03:49:09) [015] {Tr} loss: 0.220795 {Va} loss: 2.124062 acc: 25.01%\n",
            "(2019-06-14 03:52:52) [016] {Tr} loss: 0.220225 {Va} loss: 2.114804 acc: 24.02%\n",
            "(2019-06-14 03:56:35) [017] {Tr} loss: 0.220065 {Va} loss: 2.111554 acc: 21.47%\n",
            "(2019-06-14 04:00:18) [018] {Tr} loss: 0.220126 {Va} loss: 2.104219 acc: 25.58%\n",
            "(2019-06-14 04:04:01) [019] {Tr} loss: 0.220218 {Va} loss: 2.118489 acc: 24.51%\n",
            "(2019-06-14 04:07:44) [020] {Tr} loss: 0.219740 {Va} loss: 2.112098 acc: 23.03%\n",
            "(2019-06-14 04:11:27) [021] {Tr} loss: 0.221364 {Va} loss: 2.118505 acc: 23.34%\n",
            "(2019-06-14 04:15:10) [022] {Tr} loss: 0.221235 {Va} loss: 2.117602 acc: 23.53%\n",
            "(2019-06-14 04:18:53) [023] {Tr} loss: 0.220727 {Va} loss: 2.118522 acc: 26.68%\n",
            "(2019-06-14 04:22:36) [024] {Tr} loss: 0.221500 {Va} loss: 2.124278 acc: 26.51%\n",
            "(2019-06-14 04:26:20) [025] {Tr} loss: 0.221015 {Va} loss: 2.117985 acc: 23.75%\n",
            "(2019-06-14 04:30:08) [026] {Tr} loss: 0.220426 {Va} loss: 2.107048 acc: 24.29%\n",
            "(2019-06-14 04:33:55) [027] {Tr} loss: 0.221032 {Va} loss: 2.124709 acc: 26.17%\n",
            "(2019-06-14 04:37:42) [028] {Tr} loss: 0.220714 {Va} loss: 2.102480 acc: 25.01%\n",
            "(2019-06-14 04:41:29) [029] {Tr} loss: 0.220033 {Va} loss: 2.121897 acc: 24.32%\n",
            "(2019-06-14 04:45:16) [030] {Tr} loss: 0.220142 {Va} loss: 2.113551 acc: 25.99%\n",
            "(2019-06-14 04:49:03) [031] {Tr} loss: 0.222408 {Va} loss: 2.126340 acc: 24.31%\n",
            "(2019-06-14 04:52:53) [032] {Tr} loss: 0.221961 {Va} loss: 2.141240 acc: 23.81%\n",
            "(2019-06-14 04:56:43) [033] {Tr} loss: 0.221563 {Va} loss: 2.142940 acc: 25.64%\n",
            "(2019-06-14 05:00:33) [034] {Tr} loss: 0.222227 {Va} loss: 2.142291 acc: 25.50%\n",
            "(2019-06-14 05:04:28) [035] {Tr} loss: 0.222098 {Va} loss: 2.141550 acc: 26.30%\n",
            "(2019-06-14 05:08:24) [036] {Tr} loss: 0.222303 {Va} loss: 2.155502 acc: 24.64%\n",
            "(2019-06-14 05:12:22) [037] {Tr} loss: 0.222200 {Va} loss: 2.153479 acc: 22.45%\n",
            "(2019-06-14 05:16:21) [038] {Tr} loss: 0.222018 {Va} loss: 2.140084 acc: 27.33%\n",
            "(2019-06-14 05:20:22) [039] {Tr} loss: 0.222202 {Va} loss: 2.160606 acc: 23.05%\n",
            "(2019-06-14 05:24:23) [040] {Tr} loss: 0.221443 {Va} loss: 2.146150 acc: 23.58%\n",
            "(2019-06-14 05:28:24) [041] {Tr} loss: 0.223981 {Va} loss: 2.167973 acc: 24.06%\n",
            "(2019-06-14 05:32:22) [042] {Tr} loss: 0.223695 {Va} loss: 2.170560 acc: 23.08%\n",
            "(2019-06-14 05:36:21) [043] {Tr} loss: 0.223920 {Va} loss: 2.176570 acc: 24.35%\n",
            "(2019-06-14 05:40:21) [044] {Tr} loss: 0.224635 {Va} loss: 2.248381 acc: 24.92%\n",
            "(2019-06-14 05:44:21) [045] {Tr} loss: 0.225804 {Va} loss: 2.181915 acc: 21.97%\n",
            "(2019-06-14 05:48:24) [046] {Tr} loss: 0.224106 {Va} loss: 2.179331 acc: 25.35%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}